---
title: "Project_4"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_section: true
date: "2023-11-18"
---
```{r setup, include=F}
knitr::opts_chunk$set(echo = T, 
                      include = T, 
                      warning = F)
```
```{r, echo = F, message = F, warning = F, include = F}
if (!require('dplyr')){
  install.packages('dplyr')
}
if (!require('ggplot2')){
  install.packages('ggplot2')
}
if (!require('ggcorrplot')){
  install.packages('ggcorrplot')
}
if (!require('corrplot')){
  install.packages('corrplot')
}
if (!require('car')){
  install.packages('car')
}
if (!require('GGally')){
  install.packages('GGally')
}
if (!require('multcomp')){
  install.packages('multcomp')
}
if (!require('readxl')){
  install.packages('readxl')
}
if (!require('vegan')){
  install.packages('vegan')
}
if (!require('factoextra')){
  install.packages('factoextra')
}
if (!require('scatterplot3d')){
  install.packages('scatterplot3d')
}
if (!require('rgl')){
  install.packages('rgl')
}
if (!require('gridExtra')){
  install.packages('gridExtra')
}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
if (!require('DESeq2')){
  BiocManager::install("DESeq2")
}
if (!require('limma')){
  BiocManager::install("limma")
}
if (!require(rgl)){
  install.packages("rgl")
  }
```

Libraries used:

```{r, echo = T}
library(readxl)
library(ggplot2)
library(dplyr)
library(car)
library(multcomp)
library(GGally)
library(corrplot)
library(ggcorrplot)
library(vegan)
library(factoextra)
library(scatterplot3d)
library(rgl)
library(gridExtra)
library('BiocManager')
library(limma)
library(rgl)
```

```{r, echo = F}
theme_set(theme_bw())
```

# Introduction 

For this project I will use data from the article abouthow Down syndrome affects the levels of various proteins. Data for this project can be found [here](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression#).

*Goal* -- conduct EDA, make a few linear models to predict production level of some proteins and also make PCA.
In additional part of this project I will analyze differential expressing genes.


# Data description

```{r}
mouse_data <- read_xls('Data/Data_Cortex_Nuclear.xls')
str(mouse_data)
```

We can see that MouseID contain information both about mouse id and about technical repetition. To create new variable **id** in data I made this function:

```{r}
column_ID <- function(df, column_name){
  df <- df %>% mutate(id = sub("_.*", "", .[[column_name]]))
  return(df)
}
```

```{r}
mouse_data <- column_ID(mouse_data, 'MouseID')
mouse_data$id <- as.numeric(mouse_data$id)
```

There are `r length(unique(mouse_data$id))` mice in experiment. It may be a good idea to make some of our variables as factors.

```{r}
mouse_data$Genotype <- as.factor(mouse_data$Genotype)
mouse_data$Treatment <- as.factor(mouse_data$Treatment)
mouse_data$Behavior <- as.factor(mouse_data$Behavior)
mouse_data$class <- as.factor(mouse_data$class)
classes <- levels(mouse_data$class)
```

We can distinguish `r length(classes)` groups by variable **class**. 


```{r}
class_pivot_count <- mouse_data %>%
                      group_by(class) %>% 
                      summarise(count = n() / 15)
treat_pivot_count <- mouse_data %>%
                     group_by(Treatment) %>% 
                     summarise(count = n() / 15)
genotype_pivot_count <- mouse_data %>%
                        group_by(Genotype) %>% 
                        summarise(count = n() / 15)
behavior_pivot_count <- mouse_data %>%
                        group_by(Behavior) %>% 
                        summarise(count = n() / 15)
```   




```{r, echo=FALSE, fig.width = 10, fig.height = 8}
group_distr_col <- function(data, x, y = 'count'){
  ggplot(data, aes(x = list(data[x][[1]])[[1]], y = list(data[y][[1]])[[1]])) +
    geom_col(fill = '#FDC65E') +
    theme(plot.title = element_text(hjust = 0.5)) + 
    xlab(paste0('Groups based on variable ', x)) + 
    ylab('Number of mice') +
    ggtitle(paste0('Distribution of mice\nin groups based on variable ', x)) +
    scale_x_discrete(guide = guide_axis(angle = 30))
}

grid.arrange(
            group_distr_col(class_pivot_count, 'class'),
            group_distr_col(behavior_pivot_count, 'Behavior'),
            group_distr_col(treat_pivot_count, 'Treatment'),
            group_distr_col(genotype_pivot_count, 'Genotype'),
            ncol = 2)

```

We can see that the groups are not quite balanced and ANOVA that we plan to apply further may be less resistant to violation of the conditions of applicability.

Classes:

- c-CS-s: control mice, stimulated to learn, injected with saline (9 mice)
- c-CS-m: control mice, stimulated to learn, injected with memantine (10 mice)
- c-SC-s: control mice, not stimulated to learn, injected with saline (9 mice)
- c-SC-m: control mice, not stimulated to learn, injected with memantine (10 mice)
- t-CS-s: trisomy mice, stimulated to learn, injected with saline (7 mice)
- t-CS-m: trisomy mice, stimulated to learn, injected with memantine (9 mice)
- t-SC-s: trisomy mice, not stimulated to learn, injected with saline (9 mice)
- t-SC-m: trisomy mice, not stimulated to learn, injected with memantine (9 mice)


## Making data tidy

Now there is a time to deal with NA. There are `r sum(is.na(mouse_data))` missing values in our dataset. It is interesting to check distribution of NAs in different groups. May be some proteins just were not expressed in certain conditions.


```{r, echo = F, include = F}
na_data <- apply(apply(mouse_data, 2, is.na), 2, sum)
```

Since there are quite a few missing values in the data, I will replace them with group means (class variable).

```{r}
mouse_data_wo_na <- mouse_data %>% group_by(class) %>% mutate(across(everything(), function(x) ifelse(is.na(x), mean(x, na.rm = T), x)))
mouse_data_wo_na$Genotype <- mouse_data$Genotype
mouse_data_wo_na$Treatment <- mouse_data$Treatment
mouse_data_wo_na$Behavior <- mouse_data$Behavior
```

# BDNF_N production

To determine if there is difference in BDNF expression level between different classes I conduct ANOVA.


```{r}
bdnf_mod <- lm(BDNF_N ~ class, data = mouse_data_wo_na)
av_bdn_mod <- Anova(bdnf_mod)
av_bdn_mod
```
We can see that **class** is a significant predictor. So we can assume that there is some difference in the BDNF_N production level depending on the class in experiment. But first we have to check the conditions of ANOVA applicability and conduct post-hoc tests.

## Checking the applicability conditions 

```{r}
bdnf_mod_diag <- fortify(bdnf_mod)
```

**Check the independence of observations**


There are several heuristics for calculating threshold for Cook's distances. It can be just 2, 3 * $\bar{y}$ or $\frac4n$, where n is number of samples. Here I am going to use the second

```{r}
cook_threshold <- 3 * mean(bdnf_mod_diag$BDNF_N)
```


```{r}
ggplot(bdnf_mod_diag, aes(x = 1:nrow(bdnf_mod_diag), y = .cooksd)) +
  geom_bar(stat = 'identity') +
  geom_hline(yintercept = cook_threshold, color = "red") + 
  xlab("Samle number") + 
  ylab("Cook's distance") +
  ggtitle("Graph of Cook's distances") +
  theme(plot.title = element_text(hjust = 0.5))
```

There are no samples which demonstrate bigger values than threshold.

**Model residues**

Residuals plots show that there are quite a few poorly predicted values, but no particular pattern is observed, also the median value of residuals in different classes is approximately the same and is close to zero.

```{r}
ggplot(data = bdnf_mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  xlab('Prediction') + 
  ylab('Standardized residuals') +
  ggtitle('Distribution of standardized residuals of model\nin fitted values') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(bdnf_mod_diag, aes(x = class, y = .stdresid)) +
  geom_boxplot() + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  xlab('Class') + 
  ylab('Standardized residuals') +
  ggtitle('Distribution of standardized residuals of model\nin class')
```

**Normal distribution of model residuals**

We can't name the distribution of the residuals of the model a normal, but we may try to apply ANOVA because other conditions generally applicable and also we have lots of samples that is good for ANOVA.

```{r}
qqPlot(bdnf_mod, xlab = 'Normal distribution quantiles', ylab = 'Quantiles of the distribution of the model residuals')
shapiro.test(bdnf_mod_diag$.resid)
```

**Post-hoc tests**

Here I will use Tukey post-hoc test:

```{r}
res_tukey <- glht(bdnf_mod, linfct = mcp(class = 'Tukey'))
summary(res_tukey)
```

**Post-hoc tests visualization**

```{r}
data <-  expand.grid(class = mouse_data_wo_na$class)
data <- data.frame(data,
                   predict(bdnf_mod, newdata = data, interval = 'confidence'))
pos <- position_dodge(width = 0.2)
gg_linep <- ggplot(data, aes(x = class, y = fit,
                             ymin = lwr, ymax = upr)) + 
  geom_point(position = pos) +
  geom_errorbar(position = pos, width = 0.2) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  xlab('Class') + 
  ylab('Predicted mean') +
  ggtitle('Dependence of the predicted\nmean expression of BDNF_N in the class')
gg_linep
```

It can be seen that there is a significant dependence of the BDNF_N production in the class in the experiment.

BDNF belongs to neurotrophins, substances that stimulate and support the development of neurons. BDNF acts on certain neurons in the central and peripheral nervous systems, helping emerging neurons survive, and increases the number and differentiation of new neurons and synapses. Based on the results of post-hoc tests, the following conclusions can be drawn:

1. In the memantine applicable control mice, BDNF production was significantly higher when stimulated to learn. The same effect is observed in the group of saline control mice
2. The learning stimulation causes a significantly higher level of BDNF production in the case of control mice (compared to mice with trisomy)
3. Unstimulated mice with trisomy which were treated with memantine have significantly higher level of BDNF_N production than the same control mice


# ERBB4_N production

```{r, fig.width = 7, fig.height = 7}
exp_data <- mouse_data_wo_na[, -c(1:6)]
ggcorr(exp_data, hjust = 0.75, size = 1.5, color = "black", layout.exp = 1)
```

Judging by this heat-map, we can expect the presence of multicollinearity in the data, but this still has to be checked.
I am going to build a complete linear model for the ERBB4_N protein.

```{r}
erbb_mod <- lm(ERBB4_N ~ ., data = exp_data)
```

## Model diagnostics

```{r}
erbb_mod_diag <- fortify(erbb_mod)
```

```{r, include = F}
erbb_threshold <- 3 * mean(erbb_mod_diag$ERBB4_N)
```

**Graph of Cook's distances**

There are no influential observations.

```{r}
ggplot(erbb_mod_diag, aes(x = 1:nrow(erbb_mod_diag), y = .cooksd)) +
  geom_bar(stat = 'identity') +
  xlab('Samle number') + 
  ylab("Cook's distance") +
  ggtitle("Graph of Cook's distances") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_hline(yintercept = erbb_threshold, color = "red")
```

**Model residues**

The residuals have a pronounced pattern and many observations are lagging behind by more than 2 standard deviations.

```{r}
ggplot(data = erbb_mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  xlab('Prediction') + 
  ylab('Standardized residuals') +
  ggtitle('Distribution of standardized residuals of model\nin fitted values') + 
  theme(plot.title = element_text(hjust = 0.5))
```

**Normal distribution of model residuals**

Distribution of standardized residues differ significantly from normal. 

```{r}
qqPlot(erbb_mod)
shapiro.test(erbb_mod_diag$.stdresid)
```

**Checking for multicollinearity**

Multicollinearity is likely to exist in our data, this is also clear from the point of view of biology, since many proteins form metabolic pathways, and, accordingly, are expressed together.

```{r, include = F}
alias(erbb_mod)
```

The data exhibit complete collinearity, which causes the VIF function to fail unless an additional parameter is specified. To address this, you can identify the variable causing the issue (in this case, ARC_N) and remove it from the model. However, even after removing this variable, the VIF values for most predictors remain excessively high. This suggests that such a model is inherently flawed.

Many assumptions necessary for the application of linear models are violated, particularly multicollinearity. While it is possible to reduce multicollinearity by eliminating predictors with high VIF values, this would require removing a significant number of variables, and there is no guarantee that the resulting model would be effective with fewer predictors. In such situations, dimension reduction techniques might be a better alternative.

# PCA

## Calculation of principal components

Let's make a dataframe in which each observation is the average protein expression for each mouse over replicates.
```{r}
mouse_wo_reps <- mouse_data_wo_na[, -c(1, 79, 80, 81, 82)] %>% group_by(id) %>% summarise_all('mean')
mouse_wo_reps <- arrange(mouse_wo_reps, id)
cl <- arrange(mouse_data_wo_na[!duplicated(mouse_data_wo_na$id), ], id)$class
gn <- arrange(mouse_data_wo_na[!duplicated(mouse_data_wo_na$id), ], id)$Genotype
tr <- arrange(mouse_data_wo_na[!duplicated(mouse_data_wo_na$id), ], id)$Treatment
bh <- arrange(mouse_data_wo_na[!duplicated(mouse_data_wo_na$id), ], id)$Behavior

mouse_wo_reps$class <- cl
mouse_wo_reps$Genotype <- gn
mouse_wo_reps$Treatment <- tr
mouse_wo_reps$Behavior <- bh
```

```{r}
mouse_wo_reps_pca <- rda(mouse_wo_reps[, -c(1, 79, 80, 81, 82)], scale = T)
```


**Factor loadings**

The angles between the vectors reflect the correlations of features with each other and with the axes of the principal components. With a large number of variables, it is quite difficult to interpret this graph, but still it can be seen that most of the variables have a positive correlation with PC1.

```{r}
biplot(mouse_wo_reps_pca, scaling = 'species', display = 'species', main = 'Correlation biplot (loads graph)')
```

**Distance biplot**

Distances between points (objects - mice) approximate Euclidean distances and reflect the similarity between objects - mice.

```{r}
biplot(mouse_wo_reps_pca, scaling = 'sites', display = 'sites', main = 'Distance biplot (ordination plot)', type = 'points')
```

**Ordination plot using ggplot**

Graph of ordination in the axes of the first two principal components:

```{r, fig.width = 12, fig.height = 12}
df_scores <- data.frame(mouse_wo_reps,
                        scores(mouse_wo_reps_pca, display = "sites", choices = c(1, 2, 3), scaling = "sites"))
cl <- ggplot(df_scores, aes(x = PC1, y = PC2)) + 
      geom_point(aes(color = class), alpha = 0.5) +
      coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Principal component axis ordination") + theme_bw() +
      theme(plot.title = element_text(hjust = 0.5))
fst <- ggplot(df_scores, aes(x = PC1, y = PC2)) + 
      geom_point(aes(color = Treatment), alpha = 0.5) +
      coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Principal component axis ordination") + theme_bw() +
      theme(plot.title = element_text(hjust = 0.5))
sec <- ggplot(df_scores, aes(x = PC1, y = PC2)) + 
      geom_point(aes(color = Genotype), alpha = 0.5) +
      coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Principal component axis ordination") + theme_bw() +
      theme(plot.title = element_text(hjust = 0.5))
thr <- ggplot(df_scores, aes(x = PC1, y = PC2)) + 
      geom_point(aes(color = Behavior), alpha = 0.5) +
      coord_equal(xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2)) + ggtitle(label = "Principal component axis ordination") + theme_bw() +
      theme(plot.title = element_text(hjust = 0.5))

grid.arrange(cl, fst, sec, thr, ncol = 2)
```

Interestingly, according to the Genotype and Treatment variables, mice are poorly separated in the space of the first two components, and according to the Behavior variable, yes, learning probably has the strongest effect on protein production in the data.

## PCA interpretation

### Number of components

At the very beginning, we need to understand how each component contributes.


```{r}
screeplot(mouse_wo_reps_pca, type = "lines", bstick = F, main = 'Eigenvalues plot')
```

Another good way is to see what percentage of variance each component explains. It is usually sufficient that the total percentage of variability of the components is about 90 %. It can be seen from the summary model that for this it is necessary to use 14 principal components.

```{r}
pca_summary <- summary(mouse_wo_reps_pca)
pca_result <- as.data.frame(pca_summary$cont)
pca_result <- pca_result[, 1:9]
plot_data <- as.data.frame(t(pca_result[c("Proportion Explained"),]))
plot_data$component <- rownames(plot_data)
plot_data$component <- as.factor(plot_data$component)
ggplot(plot_data, aes(component, y = `Proportion Explained`)) + 
  geom_bar(stat = "identity") + 
  theme_bw() + 
  ggtitle(label = "Explained variance") + theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_discrete(guide = guide_axis(angle = 30))
  
```

# ERBB4_N production via PCA

Build a new model using 14 first PC.

```{r}
mouse_wo_erbb <- mouse_wo_reps[, -c(1, 56, 79, 80, 81, 82)]
mouse_erbb_pca <- rda(mouse_wo_erbb, scale = T)
mouse_for_model <- data.frame(ERBB4_N = mouse_wo_reps$ERBB4_N, scores(mouse_wo_reps_pca, display = "sites", choices = 1:14, scaling = "sites"))
model_via_pca <- lm(ERBB4_N ~ ., data = mouse_for_model)
summary(model_via_pca)
```

## Model diagnostics

```{r}
erbb_mod_diag <- fortify(model_via_pca)
```

```{r, include = F}
erbb_threshold <- 3 * mean(erbb_mod_diag$ERBB4_N)
```


**Graph of Cook's distances**

Here we can conclude that there is no influential observations in this case.

```{r}
ggplot(erbb_mod_diag, aes(x = 1:nrow(erbb_mod_diag), y = .cooksd)) +
  geom_bar(stat = 'identity') +
  xlab('Samle number') + 
  ylab("Cook's distance") +
  ggtitle("Graph of Cook's distances") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_hline(yintercept = erbb_threshold, color = "red")
```

**Model residues**

Distribution of standardized residuals of model also looks good.

```{r}
ggplot(data = erbb_mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  xlab('Prediction') + 
  ylab('Standardized residuals') +
  ggtitle('Distribution of standardized residuals of model\nin fitted values') + 
  theme(plot.title = element_text(hjust = 0.5))
```

**Normal distribution of model residuals**

Distribution of standardized residues does not differ significantly from a normal. 

```{r}
qqPlot(erbb_mod)
shapiro.test(erbb_mod_diag$.stdresid)
```

In general, it is clear that this model is already quite good. It would also be possible to select only significant predictors using a partial F-test, but since we will not predict anything further, this will remain outside the scope of this work.